{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32059a03-c3d1-4dd7-9667-c85ff060e0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Precision_max_avg  Precision_max_avg_index  Recall_max_avg  \\\n",
      "ml_algorithm                                                               \n",
      "A                      0.629823                    13112        0.864675   \n",
      "B                      0.568537                    51856        0.815395   \n",
      "\n",
      "              Recall_max_avg_index  \n",
      "ml_algorithm                        \n",
      "A                            39038  \n",
      "B                            51856  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "########################################################################\n",
    "# Find the highest Macro-averaged Precision and Recall (id and value)  #\n",
    "#  for the each machine learning algorithm used and target variable    #\n",
    "# -----------------------------  Tenure  ------------------------------#\n",
    "########################################################################\n",
    "\n",
    "df = pd.read_excel(\"performance_results\\excell_performance_results_tenure_algs.xlsx\")\n",
    "metrics = ['Precision', 'Recall']\n",
    "df_max_avgs_and_ids = pd.DataFrame(columns=[f'{metrics[0]}_max_avg', \n",
    "                                            f'{metrics[0]}_max_avg_index', \n",
    "                                            f'{metrics[1]}_max_avg', \n",
    "                                            f'{metrics[1]}_max_avg_index'])\n",
    "indexes = [] # \"Non-NaN\" indexes in Precision\n",
    "for metric in metrics:\n",
    "    df_ids_know_all_classes = df.copy(deep=True)\n",
    "\n",
    "    # Get indexes of Precision and Recall that acknowledge the existence of #\n",
    "    #  all classes                                                          #\n",
    "    #\n",
    "    # (Recall) check only the rows that in 'Precision' have no nans (meaning\n",
    "    #  the training set, and thus the model, \"knows\" all 3 classes, at least as\n",
    "    #   False Positives).\n",
    "    if metric == metrics[1]:\n",
    "        #* Note: 'indexes' will have values from previous iterations\n",
    "        df_ids_know_all_classes = df.loc[indexes] \n",
    "    # (Precision) Get rows whose Precision captured 3 results on its \"list\"\n",
    "    #  (it's actually a string)  \n",
    "    if metric == metrics[0]: \n",
    "        df_ids_know_all_classes = df_ids_know_all_classes[df_ids_know_all_classes[metric].apply(lambda x: len(str(x).strip(\"[]\").split()) == 3)]\n",
    "    # Remove the uncessary spaces and brackets and split the string into \n",
    "    #  3 strings (the values)\n",
    "    df_ids_know_all_classes[metric] = df_ids_know_all_classes[metric].apply(lambda x: str(x).strip(\"[]\").split())\n",
    "    # (Precision) Remove those that contain a 'nan'\n",
    "    if metric == metrics[0]:\n",
    "        df_ids_know_all_classes = df_ids_know_all_classes[df_ids_know_all_classes[metric].apply(lambda x: 'nan' not in x)]\n",
    "        indexes = df_ids_know_all_classes.index.to_list()\n",
    "    # (Recall) Replace those that contain a 'nan' by 0 \n",
    "    #  This is because if 'Precision' acknowldged them, then despite there\n",
    "    #  being no False Negatives, all classes were acknowledged by the model, \n",
    "    #  at least as False Positives. Thus, it's fair to use that entry, and also \n",
    "    #  to only divide the result by 2, since TP and FN are impossible to measure\n",
    "    #  in the cases the test set did not have any instances of one of the classes.\n",
    "    elif metric == metrics[1]:\n",
    "        df_ids_know_all_classes[metric] = df_ids_know_all_classes[metric].apply(\n",
    "            lambda x: [string.replace('nan', '0') for string in x]\n",
    "        )\n",
    "        \n",
    "    # Convert list of strings to list of floats\n",
    "    df_ids_know_all_classes[metric] = df_ids_know_all_classes[metric].apply(\n",
    "        lambda x: [float(str_float) for str_float in x]\n",
    "    ) \n",
    "    # Get mean of the list of floats\n",
    "    df_means = df_ids_know_all_classes[['ml_algorithm', metric]].copy()\n",
    "    if metric == metrics[0]:\n",
    "        df_means[metric] = df_ids_know_all_classes[metric].apply(np.mean)\n",
    "    elif metric == metrics[1]: \n",
    "        # (Recall) Divide by len(x)-1 since it would be \"unfair\" to account for \n",
    "        # the \"Share of freehold\" 0\n",
    "        df_means[metric] = df_ids_know_all_classes[metric].apply(lambda x: sum(x) / (len(x)-1))\n",
    "        \n",
    "    # Get id of the max mean for each ml_algorithm\n",
    "    df_max_avgs_and_ids[f'{metric}_max_avg'] = df_means.groupby(by=['ml_algorithm']).max()\n",
    "    df_max_avgs_and_ids[f'{metric}_max_avg_index'] = df_means.groupby(by=['ml_algorithm']).idxmax()\n",
    "\n",
    "# Print best averages and respective indexes \n",
    "#  by machine learning method\n",
    "print(df_max_avgs_and_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebScrapProj_RealEstate",
   "language": "python",
   "name": "webscrapproj_realestate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
