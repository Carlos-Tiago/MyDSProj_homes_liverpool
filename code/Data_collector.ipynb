{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b5ee5-6b80-4c74-9aa8-af6c7eb5aae8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install -U beautifulsoup4\n",
    "%pip install -U selenium\n",
    "%pip install -U pandas\n",
    "%pip install -U matplotlib\n",
    "%pip install -U psycopg2 \n",
    "%pip install -U pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90765cb-66ea-49e6-a9d9-340668e7a207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "#from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import requests\n",
    "import re #regex\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "#makes it so all modules are reloaded, allowing...\n",
    "#...me to not restart the kernel when they are edited!\n",
    "%load_ext autoreload \n",
    "%autoreload 2  \n",
    "from CallPostgre import Database #custom module for db commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dae5bb-4a65-44ed-8684-917264349247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getSoupStartingPage(url, chrome_options, current_page_number) -> BeautifulSoup:\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(url)  \n",
    "\n",
    "    # Load the page when a certain attribute is found, or after a set amount of time\n",
    "    # Helps ensuring the driver has the fully loaded page content. \n",
    "    # Also mimics normal browser behaviour in case a scrapping verification that\n",
    "    # takes (say) 5 seconds exists.\n",
    "    max_wait_time = 6\n",
    "    try:\n",
    "        WebDriverWait(driver, max_wait_time).until(EC.presence_of_element_located((By.XPATH, f\"//link[@href='https://www.zoopla.co.uk/for-sale/property/liverpool/?pn={current_page_number}']\")))\n",
    "        print (f\"Page loaded after finding the desired content!\")\n",
    "    except TimeoutException:\n",
    "        print (f\"Page loaded by timeout... ({max_wait_time} seconds)\")\n",
    "    \n",
    "    html = driver.page_source\n",
    "    \n",
    "    # Drivers must be closed otherwise the process of the connection started\n",
    "    # will keep running in the background, leading to many processes stacking\n",
    "    # up as the program is ran multiple times, draining my RAM (and CPU) memory\n",
    "    # until an error starts triggering.\n",
    "    # PS: works here but not at the end of the program, not yet sure why.\n",
    "    driver.quit()  \n",
    "    \n",
    "    soup = BeautifulSoup(html)\n",
    "    return soup\n",
    "    \n",
    "    \n",
    "def getID(url_homedetails_portion) -> int:\n",
    "    regex_home_id = re.compile(\"(?:.+details\\/)(\\d+)(?:\\/)\")\n",
    "    home_id = regex_home_id.search(url_homedetails_portion).group(1)\n",
    "    return int(home_id)\n",
    "\n",
    "\n",
    "def getPrice() -> int:\n",
    "    elem_price = soup_home.find(attrs={\"data-testid\":\"listing-price\"})\n",
    "    price = elem_price.string\n",
    "    # 'r' makes it so the parameter is read as a regex, \n",
    "    #  which makes it very slightly faster\n",
    "    price = price.replace(\"Â£\",r\"\").replace(\",\",r\"\") \n",
    "    return int(price)\n",
    "\n",
    "\n",
    "\n",
    "def getRoomCountsAndPropertySize() -> tuple[list[int|None], int | None]: \n",
    "    elem_room_counts = soup_home.find_all(class_=re.compile(\"^_1qv6swd1$\")) #elem->html element\n",
    "    # Able to save room for a possible \"square meters\" information, \n",
    "    # yet it is almost never present in listings, so, for the time being, \n",
    "    # that value will be ignored.\n",
    "    room_counts_values = [None for i in range(3)]\n",
    "    room_counts_keys = [\"Bedrooms\", \"Bathrooms\", \"Living rooms\"]\n",
    "    property_size = None\n",
    "    \n",
    "    for elem_room_count in elem_room_counts: \n",
    "        if elem_room_count.contents[0].string == \"Property size\":\n",
    "            property_size = re.search(\"^[^\\d]*(\\d+)\" , elem_room_count.contents[2].string).group(1)\n",
    "        else:\n",
    "            # get the name of the division\n",
    "            for i, room_counts_key in enumerate(room_counts_keys):\n",
    "                # when the website matches one of those names, update that index\n",
    "                if elem_room_count.contents[0].string == room_counts_key:\n",
    "                    # regular expression retrieves the first number found,\n",
    "                    # covering the \"n sq. ft case\" too\n",
    "                    room_counts_values[i] = elem_room_count.contents[2].string\n",
    "                    break\n",
    "    #Convert from string to int, so they can be stored in the db     \n",
    "    room_counts = [int(n) if n is not None else None for n in room_counts_values]\n",
    "    \n",
    "    if property_size is not None:\n",
    "        property_size = int(property_size)\n",
    "    return room_counts, property_size\n",
    "\n",
    "\n",
    "def getTitleAndLocation() -> tuple[str, str]:\n",
    "    elem_description = soup_home.find(class_=re.compile(\"^_1vvnr3j0$\"))  \n",
    "    title = elem_description.contents[0].string\n",
    "    location = elem_description.contents[1].string\n",
    "    return title, location\n",
    "\n",
    "\n",
    "def getListingDate() -> str:\n",
    "    datestring_unprocessed = soup_home.find(class_=re.compile(\"^_65yptp1$\")).string\n",
    "    regexdate = re.compile(\"(3[01]|[0-2][0-9]|[0-9])(?:st|nd|rd|th) ([jJ]an(uary)?|[fF]eb(ruary)?|[mM]ar(ch)?|[aA]pr(il)?|[mM]ay|[jJ]un(e)?|[jJ]ul(y)?|[aA]ug(ust)?|[sS]ep(tember)?|[oO]ct(ober)?|[nN]ov(ember)?|[dD]ec(ember)?) (2[0-9][0-9][0-9])\") \n",
    "    regexmatch = regexdate.search(datestring_unprocessed)\n",
    "    datestringcapture = regexmatch.groups() \n",
    "    datestring = \" \".join([strg for strg in reversed(datestringcapture) if strg is not None])  \n",
    "    # parse the date, and format it into 'YYYY-MM-DD' \n",
    "    dateformatted = datetime.datetime.strptime(datestring, '%Y %b %d') \n",
    "    return dateformatted\n",
    "\n",
    "\n",
    "def getHasPriceInfluences() -> tuple[bool, bool]:\n",
    "    is_auction = False\n",
    "    is_shared_ownership = False\n",
    "    \n",
    "    try:\n",
    "        elem_tags = soup_home.find(class_=re.compile(\"^_1rnkq5r0$\"))\n",
    "        for elem_tag in elem_tags:\n",
    "            tag = elem_tag.find(class_=re.compile(\"^_1p8nftv0$\")).string\n",
    "            if tag.casefold() == \"auction\":\n",
    "                is_auction = True\n",
    "            # -There is a slight risk in this method, happening if the website\n",
    "            # creates another tag with the word \"shared\".\n",
    "            # -However the fact that seems unlikely plus the risk of one of the \n",
    "            # developers simply changing the wording in \"shared ownership\"\n",
    "            # to anything else with the word \"shared\", made this, in my opinion,\n",
    "            # be the more resilient choice.\n",
    "            elif \"shared\" in tag.casefold():\n",
    "                is_shared_ownership = True\n",
    "        return is_auction, is_shared_ownership\n",
    "    \n",
    "    #If there's no tag on the house we assume it's not an auction or shared ownership.\n",
    "    finally:\n",
    "        return is_auction, is_shared_ownership\n",
    "\n",
    "    \n",
    "def getSoupHomeDetails(url_homedetails_portion, chrome_options) -> BeautifulSoup:\n",
    "    url_homedetails = \"https://www.zoopla.co.uk\"+ url_homedetails_portion \n",
    "    print(f\"\\nurl_homedetails: {url_homedetails}\")\n",
    "    driver = webdriver.Chrome(options=chrome_options) \n",
    "    driver.get(url_homedetails)\n",
    "    # Similar reasons to WebDriverWait but enforces it so as to not overload \n",
    "    # the website with requests, which may not be deemed a \"humanly\" speed\n",
    "    # to access the home details page.\n",
    "    time.sleep(5) \n",
    "    \n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    soup_homedetails = BeautifulSoup(html)\n",
    "    return soup_homedetails\n",
    "\n",
    "\n",
    "def getJsonInfo() -> tuple[str, str, str, float, float, list]:\n",
    "    jsonhtml = soup_homedetails.find(attrs={\"type\":\"application/ld+json\"})\n",
    "    # convert a valid json string into a dict\n",
    "    jsondict = json.loads(jsonhtml.contents[0]) \n",
    "    \n",
    "    real_estate_agent_name = jsondict['@graph'][1]['name']\n",
    "    postal_code = jsondict['@graph'][1]['address']['postalCode']\n",
    "    real_estate_agent_telephone = jsondict['@graph'][1]['telephone']\n",
    "    # Note: website has \"latitude\" and \"longitude\" names mixed, I believe.\n",
    "    coordinates = [jsondict['@graph'][3]['geo']['latitude'] , jsondict['@graph'][3]['geo']['longitude']] \n",
    "    photos = [jsondict['@graph'][3]['photo'][i]['contentUrl'] for i in range(len(jsondict['@graph'][3]['photo']))]\n",
    "    return real_estate_agent_name, postal_code, real_estate_agent_telephone, float(coordinates[0]), float(coordinates[1]), photos\n",
    "    \n",
    "\n",
    "def getEPCRating() -> str | None:\n",
    "    elem_epc = soup_homedetails.find(class_=re.compile(\"^_1fuu7p80 _1fuu7p85 _1dgm2fc8 _1jdsy140$\"))\n",
    "    if elem_epc != None: #if house displays epc rating\n",
    "        elem_epc = elem_epc.find(class_=re.compile(\"^_1p8nftv0$\"))\n",
    "        elem_epc.div.decompose()\n",
    "        epc_rating = elem_epc.string\n",
    "        epc_rating = epc_rating.split(\": \")[1] #get only the letter\n",
    "    else:\n",
    "        epc_rating = None\n",
    "    return epc_rating\n",
    "\n",
    "\n",
    "def getExtras() -> tuple[str|None, int|None, int|None, str|None, int|None]:\n",
    "    # Tenure, Time remaining on lease, Service charge, Council tax band, Ground rent\n",
    "    extras_values = [None for i in range(5)] \n",
    "    # Susceptible to term changes, but can't dynamically retrieve the names\n",
    "    # (to prepare for the eventuality of a term change in the website)\n",
    "    # as some houses don't have all 5 extras terms\n",
    "    extras_keys = [\"Tenure:\", \"Time remaining on lease:\", \"Service charge:\",\n",
    "                   \"Council tax band:\", \"Ground rent:\"]    \n",
    "    soup_extras = soup_homedetails.find(class_=re.compile(\"^_1k66bqh1$\"))                                     \n",
    "    \n",
    "    for elem_extra in soup_extras.find_all(class_=re.compile(\"^_1p8nftv1n _1p8nftvk _1p8nftv12$\")):       \n",
    "        # removing comment\n",
    "        for string in elem_extra(string=True): \n",
    "            if isinstance(string, Comment):\n",
    "                string.extract()\n",
    "        \n",
    "        elem_extra_key = elem_extra.contents[0] #(Ex: \"Tenure:\")\n",
    "        elem_extra_value = elem_extra.contents[1] #(Ex: \"Freehold\")\n",
    "\n",
    "        # REPLACE (word + :) strings with (word:) string #    \n",
    "        #\n",
    "        # merge the strings (word + :) separated by the comment into one,\n",
    "        # so it can be returned by .string\n",
    "        merged = \"\".join(string for string in elem_extra_key(string=True)) \n",
    "        # leave only one string in the soup (Ex: \"Tenure\") so that tag.string\n",
    "        #  can be called for tag.string.replace.\n",
    "            # Could have simply replaced the value by using\n",
    "            #  elem_extra_key.string=\"merged\". However it deletes any other content\n",
    "            #  inside such as tags and although it could have worked in this case,\n",
    "            #  the solution I used is more general and robust, despite a couple\n",
    "            #  more computations.\n",
    "        for i, string in enumerate(elem_extra_key(string=True)): \n",
    "            if  i>0:\n",
    "                string.extract()  \n",
    "        elem_extra_key.string.replace_with(merged)\n",
    "        \n",
    "        for i, extra_key in enumerate(extras_keys):\n",
    "            # when the extra name matches one of the established extra names,\n",
    "            #  update its value on the correct extras[] index\n",
    "            if elem_extra_key.string == extra_key: \n",
    "                extras_values[i] = elem_extra_value.string\n",
    "                break\n",
    "                \n",
    "    \n",
    "    #Since these are UK homes, the monetary values are given in pounds(Â£)\n",
    "    tenure = extras_values[0]\n",
    "    time_remaining_on_lease_years = extras_values[1]\n",
    "    annual_service_charge = extras_values[2]\n",
    "    council_tax_band = extras_values[3]\n",
    "    ground_rent = extras_values[4]\n",
    "    \n",
    "    # Council tax band in this website is given in a letter (A/B/C/D,E,F,G,H)\n",
    "    #  with no more text. Therefore, if there is more text than 1 character,\n",
    "    #  it's, with a good enough degree of confidence, a \"null-equivalent\"\n",
    "    #  message such as: \"A band has not yet been confirmed\" which can be discarded\n",
    "    if council_tax_band is not None and len(council_tax_band) > 1:\n",
    "        council_tax_band = None\n",
    "    \n",
    "    # Get the numericals only\n",
    "    try:\n",
    "        # Use (\"^[^\\d]*(\\d+(\\.*\\d+))\") if the website ever includes pence (floats)\n",
    "        time_remaining_on_lease_years = re.search(\"^[^\\d]*(\\d+)\", time_remaining_on_lease_years).group(1)\n",
    "        int(time_remaining_on_lease_years)\n",
    "    except:\n",
    "        # Any other description on it means it doesn't have a concrete number\n",
    "        time_remaining_on_lease_years = None\n",
    "    \n",
    "    try:\n",
    "        annual_service_charge = annual_service_charge.replace(\",\",r\"\")\n",
    "        annual_service_charge = re.search(\"^[^\\d]*(\\d+)\", annual_service_charge).group(1)\n",
    "        int(annual_service_charge)\n",
    "    except:\n",
    "        annual_service_charge = None\n",
    "    \n",
    "    try:\n",
    "        ground_rent = ground_rent.replace(\",\",r\"\")\n",
    "        ground_rent = re.search(\"^[^\\d]*(\\d+)\", ground_rent).group(1)\n",
    "        int(ground_rent)\n",
    "    except:\n",
    "        ground_rent = None\n",
    "    \n",
    "    return (tenure, time_remaining_on_lease_years, annual_service_charge, \n",
    "            council_tax_band, ground_rent)\n",
    "    \n",
    "    \n",
    "def getFeatureSet() -> list[str]:\n",
    "    feature_set = []\n",
    "    soup_featureset = soup_homedetails.find_all(class_=re.compile(\"^swbww71$\"))\n",
    "    for elem_featureset in soup_featureset:\n",
    "        feature_set.append(elem_featureset.span.string)\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def getDescription() -> str:\n",
    "    soup_descript = soup_homedetails.find(class_=re.compile(\"^ru2q7m3$\")) \n",
    "    for br in soup_descript(\"br\"):\n",
    "        # there are 2 breaks separating each paragraph so instead of double\n",
    "        # replacing with \"\\n\" I will add it below\n",
    "        br.replace_with(\"\\n\") \n",
    "    \n",
    "    description = \"\".join(string for string in soup_descript.find_all(string=True))\n",
    "    return description\n",
    "        \n",
    "\n",
    "'''\n",
    "    DISCLAIMER:\n",
    "\n",
    "    This project acknowledges Zoopla (ZPG Ltd.) which holds publicly available\n",
    "    all the data collected for this non-commercial educational project.\n",
    "    \n",
    "    After looking into \"https://www.zoopla.co.uk/robots.txt\" no disallowance \n",
    "    was found for the url directories used in this project (\"/for-sale/houses\",\n",
    "    \"/for-sale/details\" and \"/new-homes/details\") \n",
    "    \n",
    "    No data which requires preiileged access (ex: log-in), \n",
    "    sensitive data belonging to an individual or any other non publicly available\n",
    "    source was used.\n",
    "    \n",
    "    There was a further attempt to avoid any copyright infringements, by\n",
    "    ensuring the https://www.gov.uk/guidance/exceptions-to-copyright under the \n",
    "    \"Text and data mining for non-commercial research\" section was followed.\n",
    "    \n",
    "    This project was done in good-faith for educational and non-commercial reasons.\n",
    "    It MUST NOT be used in any other context. If so, the user may be liable\n",
    "    to any actions ZPG Ltd. decide to take.\n",
    "    \n",
    "    Should a party belonging to ZPG Ltd. find a disagreement with any of the claims\n",
    "    made above and consider itself aggrieved, it is welcome and encouraged to \n",
    "    contact me so it can be resolved.\n",
    "\n",
    "'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "## No main_name idiom as  it's meant to be ran as  a script! ##\n",
    "'''\n",
    "Javascript note:\n",
    "\n",
    "If the content one is looking for is dynamically generated by client-side\n",
    "javascript as the page loads, then one can only access it using a headless\n",
    "browser such as selenium.\n",
    "'''\n",
    "\n",
    "# Establish connection with the database and instantiate cursor\n",
    "db = Database()\n",
    "\n",
    "# Define a custom user agent\n",
    "user_agents =[\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n",
    "]\n",
    "\n",
    "# Checked <<mainwebsite>>/robots.txt\n",
    "# Start on page 3 due to page 1 and even 2 homes occasionally being uploaded\n",
    "#  without all info inserted yet\n",
    "starting_page_number, ending_page_number = 3, 41\n",
    "# Scanning and collecting from 10 pages each run \n",
    "for current_page_number in range(starting_page_number, ending_page_number):\n",
    "    print(f\"\"\"\n",
    "    ########################################\n",
    "    # Web scraping page: {current_page_number}\n",
    "    ########################################\"\"\")\n",
    "    \n",
    "    random_user_agent = random.choice(user_agents)\n",
    "    chrome_options = ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument(f\"--user-agent={random_user_agent}\")\n",
    "    \n",
    "    url = \"https://www.zoopla.co.uk/for-sale/property/liverpool/?q=Liverpool%2C%20Merseyside&search_source=home&pn=\" + str(current_page_number)\n",
    "    soup = getSoupStartingPage(url, chrome_options, current_page_number)\n",
    "    #Locate html of the listing sections\n",
    "    soup_homes = soup.find_all(class_=re.compile(\"^rgd66w1$\")) #len25\n",
    "    for i, soup_home in enumerate(soup_homes):\n",
    "        print(f\"\\n ----------Home: {i+1} (in this page)------------ \\n\")\n",
    "        # Used on ID and to go the home details page\n",
    "        url_homedetails_portion = soup_home.get('href')  \n",
    "\n",
    "        # Home ID #\n",
    "        home_id = getID(url_homedetails_portion)  \n",
    "\n",
    "        # Price # (Initial, as it may be slightly changed by the real estate\n",
    "        #          agents over time)\n",
    "        price = getPrice()  \n",
    "\n",
    "        # Room Counts and Property size #\n",
    "        #\n",
    "        # property_size given in sq. feet\n",
    "        room_counts, property_size = getRoomCountsAndPropertySize()     \n",
    "        bedroom_count = room_counts[0]\n",
    "        bathroom_count = room_counts[1]\n",
    "        living_room_count = room_counts[2]\n",
    "\n",
    "\n",
    "        #Title & Location #\n",
    "        title, location = getTitleAndLocation()\n",
    "\n",
    "        # Listing Date # \n",
    "        #\n",
    "        # Store date as dd/mm/yyyy string and use .to_datetime() on pandas to\n",
    "        #  convert it to datetime. \n",
    "        #  Ex: df['date'] = pd.to_datetime(df['date'], dayfirst=True). \n",
    "        #   (dayfirst ensures pandas stores it the European/nonUS way)\n",
    "        listing_date = getListingDate() \n",
    "\n",
    "        # Auction and Shared Onwership tags, which will be useful to, for instance,\n",
    "        # (not) consider their prices, due to bias #\n",
    "        is_auction, is_shared_ownership = getHasPriceInfluences()\n",
    "\n",
    "\n",
    "        print(f\"\\nID: {home_id}\")\n",
    "        print(f\"\\nPrice: {price}\")\n",
    "        print(f\"\\nTitle: {title}\")\n",
    "        print(f\"Location: {location}\")\n",
    "        print(f\"\\nBedroom Count: {bedroom_count}\")\n",
    "        print(f\"Bathroom Count: {bathroom_count}\")\n",
    "        print(f\"Living Room Count: {living_room_count}\")\n",
    "        print(f\"Property Size (sq. ft): {property_size}\")\n",
    "        print(f\"\\nListing Date: {listing_date}\") \n",
    "        print(f\"\\nIs Auction: {is_auction}\")\n",
    "        print(f\"Is Shared Ownership: {is_shared_ownership}\")\n",
    "\n",
    "        '''\n",
    "        Go deeper into the house's page\n",
    "        '''\n",
    "        # Get html of the houses' page #\n",
    "        soup_homedetails = getSoupHomeDetails(url_homedetails_portion, chrome_options)\n",
    "\n",
    "        # Get json info # (agent, postal c., phone, coords, photos' urls)\n",
    "        real_estate_agent_name, postal_code, real_estate_agent_telephone, coordinate_x, coordinate_y, photos = getJsonInfo()\n",
    "        print(f\"\\nReal Estate Agent Name: {real_estate_agent_name}\")\n",
    "        print(f\"Postal code: {postal_code}\")\n",
    "        print(f\"Real Estate Agent Telephone: {real_estate_agent_telephone}\")\n",
    "        print(f\"Coordinates: {coordinate_x}, {coordinate_y}\")\n",
    "\n",
    "        # EPC Rating #\n",
    "        epc_rating = getEPCRating()\n",
    "        print(f\"EPC Rating: {epc_rating}\")\n",
    "\n",
    "        # Extras # \n",
    "        # (Tenure, Time Remaining On Lease, Service Charge,\n",
    "        #  Council Tax Band, Ground Rent)\n",
    "        tenure, time_remaining_on_lease_years, annual_service_charge, council_tax_band, ground_rent = getExtras()\n",
    "        #p#print(f\"\\nTenure: {tenure}\")\n",
    "        #p#print(f\"Time remaining on lease (years): {time_remaining_on_lease_years}\")\n",
    "        #p#print(f\"Service charge (p/year): {annual_service_charge}\")\n",
    "        #p#print(f\"Council tax band: {council_tax_band}\")\n",
    "        #p#print(f\"Ground rent (Â£): {ground_rent}\")\n",
    "\n",
    "        # Feature Set #\n",
    "        feature_set = getFeatureSet()\n",
    "        #p#print(f\"\\nFeature Set: {feature_set}\")\n",
    "\n",
    "        # Description #\n",
    "        description = getDescription()\n",
    "        #p#print(f\"\\nDescription: {description}\")\n",
    "        \n",
    "        # Insert data in the db #\n",
    "        db.insert(home_id, price, is_auction, is_shared_ownership,\n",
    "                  title, location, coordinate_x, coordinate_y, postal_code,\n",
    "                  bedroom_count, bathroom_count, living_room_count,\n",
    "                  property_size, epc_rating, feature_set, listing_date,\n",
    "                  photos, description, tenure, time_remaining_on_lease_years,\n",
    "                  annual_service_charge, council_tax_band, ground_rent,\n",
    "                  real_estate_agent_name, real_estate_agent_telephone)\n",
    "\n",
    "print(f\"\\n End of the listing pages has been reached!\")\n",
    "\n",
    "db.disconnect()      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebScrapProj_RealEstate",
   "language": "python",
   "name": "webscrapproj_realestate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
