{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82c5d77-81c5-450b-b834-a9c7b8764d35",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install -U scikit-learn\n",
    "#%pip install -U category-encoders\n",
    "#%pip install -U seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31683174-52b9-4f86-a280-f5495f63de96",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2 import sql \n",
    "from psycopg2.extensions import AsIs\n",
    "import numpy as np\n",
    "import yaml\n",
    "import re\n",
    "import sys\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import category_encoders as ce # sklearn related\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import BoxStyle\n",
    "import seaborn as sns\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "from scipy.stats import yeojohnson\n",
    "# makes it so all modules are reloaded, allowing...\n",
    "# ...me to not restart the kernel when they are edited!\n",
    "%load_ext autoreload \n",
    "%autoreload 2 \n",
    "# Custom import(s)\n",
    "import Utility_methods as ums\n",
    "from CallPostgre import Database \n",
    "from ML_models import SupervisedLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba9715f4-3e54-4995-bfb8-9456a7b037fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_dataframes_data(dfs_to_merge):\n",
    "    for df in dfs_to_merge:\n",
    "        # From a quick look at dataFrame.dtypes, it seems only the date \n",
    "        #  (listing_date), althought already in the right format, and the\n",
    "        #  home divisions (float64 instead of int64) are mistyped,\n",
    "        #  so let's fix that\n",
    "        for column_name in df.columns:\n",
    "            #----------------\n",
    "            #Type corrections:\n",
    "            #----------------\n",
    "            #Int64 corrections#\n",
    "            # Note: due to numpy compatibility, pandas cannot store features with\n",
    "            #  missing values (NaNs) as int, therefore it stores them as floats\n",
    "            #  a workaround is casting the column to \"Nullable Integer Data\n",
    "            #  Type: Int64\"\n",
    "            #\n",
    "            # Nontheless, Int64Dtype is still not supported in some operations\n",
    "            #  such as df[column_name].dtype, giving errors such as:\n",
    "            #  \"Cannot interpret 'Int64Dtype()' as a data type\". \n",
    "            # According to  an issue open on github, the recommendation is to\n",
    "            #  keep integers columns with \"nans\" as floats, since despite not\n",
    "            #  having an ideal representation, it can perform the same operations\n",
    "            #  as if it were \"int\".\n",
    "            #\n",
    "            # If this gets fixed, uncomment the Int64 convertion code bellow\n",
    "            #  to use it as such.\n",
    "\n",
    "            #!Uncomment once Int64Dtype is integrated as the other dtypes are\n",
    "            #\n",
    "            # Check any \"counter\" column ending in \"_count\"\n",
    "            #counts_regex = re.compile(\"_count$\")\n",
    "            #if re.search(counts_regex, column_name): \n",
    "            #    #If the counter column is in float instead of int, convert\n",
    "            #    if np.issubdtype(df[column_name].dtype, float):\n",
    "            #        # Note astype('int') doesn't work when there's NaNs present\n",
    "            #        df[column_name] = df[column_name].astype('Int64') \n",
    "\n",
    "            #e# Switch to \"elif\" if the above is uncommented\n",
    "            #\n",
    "            #* An unproperly casted datetime would only be type objects so only\n",
    "            #  those need to be checked\n",
    "            if np.issubdtype(df[column_name].dtype , object): #* \n",
    "                row_number = 0\n",
    "                # Check type of the first element of each column(if NaN check \n",
    "                #  the one after)\n",
    "                value = df[column_name].iloc[row_number] \n",
    "                if isinstance(value, list):\n",
    "                    # Avoid the \"The truth value of an array with more than one\n",
    "                    #  one element is ambiguous\" error from using `.isna()` on list\n",
    "                    #\n",
    "                    # Features with lists are guaranteed to not be nan as the\n",
    "                    #  scrapping populates an \"empty list\" therefore, \"value\"\n",
    "                    #  will never be NaN in feature columns with lists, and the\n",
    "                    #  following `.isna()` check is thus protected from the \n",
    "                    #  aforementioned error.)\n",
    "                    continue\n",
    "                # Ignore nans before checking type\n",
    "                while(pd.isna(value)): \n",
    "                    row_number += 1\n",
    "                    value = df[column_name].iloc[row_number]\n",
    "                \n",
    "                ## Datetime corrections ##\n",
    "                #\n",
    "                # Convert columns which contain pandas datetime64 fomat #\n",
    "                #  YYYY-MM-DD from \"object\" to \"datetime64\"\n",
    "                #\n",
    "                # Validates days and months but not years so this code can be\n",
    "                #  used centuries in the future (or past if we travel there\n",
    "                #  someday! :)) \n",
    "                datetime_regex = re.compile(\"^[^\\d]*(\\d{4}-(1[0-2]|0[1-9])-(3[01]|[1-2][0-9]|0[1-9]))\")\n",
    "                if re.match(datetime_regex, str(value)):\n",
    "                    df[column_name] = pd.to_datetime(df[column_name])\n",
    "            \n",
    "                    #------------------------\n",
    "                    ## Feature engineering ##\n",
    "                    # (datetime)\n",
    "                    #------------------------\n",
    "                    #\n",
    "                    # Datetimes to [year] [month] [day]\n",
    "                    df[column_name+'_year'] = df[column_name].dt.year\n",
    "                    df[column_name+'_month'] = df[column_name].dt.month\n",
    "                    df[column_name+'_day'] = df[column_name].dt.day\n",
    "                    # Discard the original datetime afterwards\n",
    "                    df.drop(columns=column_name, inplace=True)\n",
    "    \n",
    "    cleaned_dfs_to_merge = dfs_to_merge\n",
    "    return cleaned_dfs_to_merge\n",
    "\n",
    "def integrate_dataframes_data(cleaned_dfs_to_merge):\n",
    "    # Edit if more tables are added in the future\n",
    "    df_real_estate_agent, df_home_liverpool = cleaned_dfs_to_merge[0], cleaned_dfs_to_merge[1]\n",
    "    integrated_df= pd.merge(df_real_estate_agent, df_home_liverpool,\n",
    "                        on='real_estate_agent_id',\n",
    "                        how='right'\n",
    "    )\n",
    "    return integrated_df\n",
    "\n",
    "\n",
    "def split_dataframe_data_train_test(integrated_df, target_variable_name, is_nan_splitting):\n",
    "    predictors_df = integrated_df.drop(columns=target_variable_name, inplace=False)\n",
    "    target_df = integrated_df[target_variable_name]\n",
    "\n",
    "    if is_nan_splitting:\n",
    "        # Splitting the dataset by nans in the target variable \n",
    "        test_df = integrated_df[integrated_df[target_variable_name].isna()]\n",
    "        X_train = predictors_df.drop(test_df.index, inplace=False) \n",
    "        y_train = target_df.drop(test_df.index, inplace=False) \n",
    "        X_test =  test_df.drop(columns=target_variable_name, inplace=False)\n",
    "        y_test = test_df[target_variable_name]\n",
    "    else:\n",
    "        # Splitting the dataset by randomly sampling\n",
    "        #  (other methods can be used)\n",
    "        X_train, X_test, y_train, y_test = ms.train_test_split(predictors_df,\n",
    "                                                               target_df,\n",
    "                                                               test_size=0.20,\n",
    "                                                               random_state=30) \n",
    "\n",
    "    # If single-target prediction, convert to (single column) dataframe format\n",
    "    #  for ease of use:\n",
    "    if type(y_train) == pd.Series:\n",
    "        y_train = y_train.to_frame()\n",
    "    if type(y_test) == pd.Series:\n",
    "        y_test = y_test.to_frame()\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test \n",
    "\n",
    "\n",
    "\n",
    "def handle_missing_dataframe_data_X(X_train, X_t, missing_data_threshold,\n",
    "                                    features_to_drop_expert_knowledge,\n",
    "                                    numeric_imputation_method_X,\n",
    "                                    categorical_imputation_method_X):\n",
    "    # Note: X_train mus thave the same features has X_test, so if expert knowledge\n",
    "    #        or missing data thresholds remove a feature in X_train, then it must\n",
    "    #        also be removed in X_test\n",
    "\n",
    "    # Drop columns not deemed useful by an expert\n",
    "    for column_to_drop_expert_knowledge in features_to_drop_expert_knowledge:\n",
    "        if column_to_drop_expert_knowledge in X_t.columns:\n",
    "            X_t=X_t.drop(columns=[column_to_drop_expert_knowledge])\n",
    "            X_train=X_train.drop(columns=[column_to_drop_expert_knowledge]) \n",
    "            \n",
    "    # Keep only columns with \"less than threshold\" ratio of missing values \n",
    "    X_t_to_fill = X_t[X_t.columns[(X_train.isna().p_mean()<missing_data_threshold)]] \n",
    "    X_train = X_train[X_train.columns[(X_train.isna().mean()<missing_data_threshold)]] \n",
    "\n",
    "    # Split Dataframe by type #\n",
    "    #\n",
    "    # Check type hierarchy at (github): numpy/numpy/core/numerictypes.py, line ~40 \n",
    "    # Check https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
    "    #  for deprecated numpy.type alliases\n",
    "    X_t_numbers_no_dates_times, X_t_numbers_dates_times, X_t_objects, X_t_bools, X_t_lists = ums.split_dataframe_data_by_type(X_t_to_fill)\n",
    "    # Have the trainset values as base for the preprocessing measures of both\n",
    "    #  train and test\n",
    "    X_train_numbers_no_dates_times, X_train_numbers_dates_times, X_train_objects, X_train_bools, X_train_lists = ums.split_dataframe_data_by_type(X_train) \n",
    "    \n",
    "    if X_t_numbers_no_dates_times.empty:\n",
    "        imputed_X_t_numbers_no_dates_times = X_t_numbers_no_dates_times\n",
    "    else:    \n",
    "        # Numeric missing data handling \n",
    "        # This excludes years/months/days etc \n",
    "        match(numeric_imputation_method_X):      \n",
    "            case 'DEL': # delete observations with missing data\n",
    "                imputed_X_t_numbers_no_dates_times = X_t_numbers_no_dates_times.dropna(inplace=False)\n",
    "                # Drop indexes of other data types so they don't impute on indexes\n",
    "                #  which no longer exist on this one\n",
    "                X_t_objects, X_t_bools, X_t_numbers_dates_times, X_t_lists = ums.drop_indexes_of_other_data_types(imputed_X_t_numbers_no_dates_times, \\\n",
    "                                                                                                                  X_t_objects, \\\n",
    "                                                                                                                  X_t_bools, \\\n",
    "                                                                                                                  X_t_numbers_dates_times, \\\n",
    "                                                                                                                  X_t_lists)\n",
    "        \n",
    "            case 1: # Mean\n",
    "                #* Default is np.array, convert to df with transform='pandas'\n",
    "                mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean').set_output(transform='pandas') #*\n",
    "                mean_imputer.fit(X_train_numbers_no_dates_times) \n",
    "                imputed_X_t_numbers_no_dates_times = mean_imputer.transform(X_t_numbers_no_dates_times)\n",
    "            \n",
    "            case 2: #Median\n",
    "                median_imputer = SimpleImputer(missing_values=np.nan, strategy='median').set_output(transform='pandas')\n",
    "                median_imputer.fit(X_train_numbers_no_dates_times) \n",
    "                imputed_X_t_numbers_no_dates_times = median_imputer.transform(X_t_numbers_no_dates_times)\n",
    "\n",
    "            case 3: #Ffill\n",
    "                # EXPERIMENTAL as it's not applicable on the test set the same\n",
    "                #  way as in the training set. Also (I think) would work better\n",
    "                #  if the dataframe could be sorted by 'price' before being\n",
    "                #  sort back to id (a little touch I think will make ffill \n",
    "                #  better, in other words, sorting by a row that is \n",
    "                #  proportionate to the features being filled). \n",
    "               \n",
    "                imputed_X_t_numbers_no_dates_times = X_t_numbers_no_dates_times.ffill(inplace=False)\n",
    "                # In case there's NaNs in the initial row(s), fill them with\n",
    "                #  \"backwawrd fill\"\n",
    "                imputed_X_t_numbers_no_dates_times.bfill(inplace=True, limit=1)\n",
    "               \n",
    "    # Categorical missing data handling #\n",
    "    if X_t_objects.empty:\n",
    "        imputed_X_t_objects = X_t_objects\n",
    "    else: \n",
    "       match(categorical_imputation_method_X):\n",
    "           case 'DEL': # delete observations with missing data\n",
    "               imputed_X_t_objects = X_t_objects.dropna(inplace=False)\n",
    "               # Drop indexes of other data types so they don't impute on indexes\n",
    "               #  which no longer exist on this one\n",
    "               imputed_X_t_numbers_no_dates_times, X_t_bools, X_t_numbers_dates_times, X_t_lists = ums.drop_indexes_of_other_data_types(imputed_X_t_objects,  \n",
    "                                                                                                                                        imputed_X_t_numbers_no_dates_times, \n",
    "                                                                                                                                        X_t_bools, \n",
    "                                                                                                                                        X_t_numbers_dates_times, \n",
    "                                                                                                                                        X_t_lists)  \n",
    "\n",
    "        \n",
    "           case 'A': #Mode imputation\n",
    "               mean_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent').set_output(transform='pandas')\n",
    "               mean_imputer.fit(X_train_objects)\n",
    "               imputed_X_t_objects = mean_imputer.transform(X_t_objects)\n",
    "  \n",
    "\n",
    "    \n",
    "    # Boolean missing data handling #\n",
    "    #\n",
    "    # Not required so far\n",
    "    imputed_X_t_bools = X_t_bools \n",
    "    # If option is 'DEL', drop indexes of  so they don't impute on,\n",
    "    #  or have got imputed, indexes which no longer exist on this one.\n",
    "    #  Can use:\n",
    "    #  `imputed_X_t_numbers_no_dates_times, X_t_numbers_dates_times, \n",
    "    #  X_t_bools, X_t_lists = \n",
    "    #  ums.drop_indexes_of_other_data_types(imputed_X_t_objects, \n",
    "    #                                        imputed_X_t_numbers_no_dates_times, \\\n",
    "    #                                        X_t_numbers_dates_times, \\\n",
    "    #                                        X_t_bools, \\\n",
    "    #                                        X_t_lists)  \n",
    "       \n",
    "    # Datetime missing data handling (for interpolation) #\n",
    "    #\n",
    "    # Not required so far\n",
    "    # Also drop indexes of other types if the option is 'DEL'\n",
    "    imputed_X_t_numbers_dates_times = X_t_numbers_dates_times\n",
    "    \n",
    "    # List missing data handling\n",
    "    #\n",
    "    # Not required so far\n",
    "    # Each list-type feature handling might require a tailored approach\n",
    "    # Also drop indexes of other types if the option is 'DEL'\n",
    "    imputed_X_t_lists = X_t_lists \n",
    " \n",
    "   \n",
    "    #Rebuild the dataset\n",
    "    imputed_X_t = pd.concat([imputed_X_t_numbers_no_dates_times,\n",
    "                             imputed_X_t_numbers_dates_times, imputed_X_t_objects,\n",
    "                             imputed_X_t_bools, imputed_X_t_lists],\n",
    "                            axis=1)\n",
    "    \n",
    "    if imputed_X_t.isna().p_sum().sum() == 0:\n",
    "        print(f\"No missing values left. Imputation successful (X)!\")\n",
    "    \n",
    "    # All train_test_split indexes are preserved up to this point\n",
    "    return imputed_X_t\n",
    "\n",
    "\n",
    "def handle_missing_dataframe_data_y(y_train, numeric_imputation_method_y, categorical_imputation_method):\n",
    "    # Split target(s) Dataframe by type ##\n",
    "    #\n",
    "    # Shouldn't be necessary for single-target prediction, but this way the algorithm stays more general.\n",
    "    # Check type hierarchy at (github): numpy/numpy/core/numerictypes.py, line ~40 \n",
    "    # Check https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations for deprecated numpy.type alliases\n",
    "    y_train_numbers_no_dates_times, y_train_numbers_dates_times, y_train_objects, y_train_bools, y_train_lists = ums.split_dataframe_data_by_type(y_train)\n",
    "\n",
    "\n",
    "    # Numeric missing data handling #\n",
    "    # This excludes years/months/days etc \n",
    "    #\n",
    "    # If empty no point in imputing\n",
    "    if y_train_numbers_no_dates_times.empty:\n",
    "        imputed_y_train_numbers_no_dates_times = y_train_numbers_no_dates_times\n",
    "    else:\n",
    "        match(numeric_imputation_method_y):  \n",
    "            case 'DEL': # delete observations with missing data\n",
    "                 imputed_y_train_numbers_no_dates_times =  y_train_numbers_no_dates_times.dropna(inplace=False)\n",
    "                 # (Only relevant if multi-target) Drop indexes of other data\n",
    "                 #  types so they don't impute on indexes which no longer exist\n",
    "                 #  on this one\n",
    "                 y_train_objects, y_train_bools, y_train_numbers_dates_times, y_train_lists = ums.drop_indexes_of_other_data_types(imputed_y_train_numbers_no_dates_times, \\\n",
    "                                                                                                                                   y_train_objects, \\\n",
    "                                                                                                                                   y_train_bools, \\\n",
    "                                                                                                                                   y_train_numbers_dates_times, \\\n",
    "                                                                                                                                   y_train_lists)                \n",
    "                         \n",
    "            case 1: # Mean\n",
    "                # Pandas method is fine as there is no data-leakege prevention \n",
    "                #  concerns (y_test won't be imputed :))\n",
    "                imputed_y_train_numbers_no_dates_times = y_train_numbers_no_dates_times.fillna(y_train_numbers_no_dates_times.mean()) \n",
    "    \n",
    "            case 2: # Median\n",
    "                imputed_y_train_numbers_no_dates_times = y_train_numbers_no_dates_times.fillna(y_train_numbers_no_dates_times.median())\n",
    "            \n",
    "            case 3: # Ffill\n",
    "                imputed_y_train_numbers_no_dates_times = y_train_numbers_no_dates_times.ffill(inplace=False)\n",
    "                # In case there's NaNs in the first rows, bfill them\n",
    "                imputed_y_train_numbers_no_dates_times.bfill(inplace=True, limit=1)\n",
    "\n",
    "\n",
    "    if y_train_objects.empty:\n",
    "        imputed_y_train_objects = y_train_objects\n",
    "    else:   \n",
    "        # Categorical missing data handling #\n",
    "        match(categorical_imputation_method):\n",
    "           case 'DEL': #delete observations with missing data\n",
    "               imputed_y_train_objects = y_train_objects.dropna(inplace=False)\n",
    "               # (Only relevant if multi-target) Drop indexes of other data\n",
    "                 #  types so they don't impute on indexes which no longer exist\n",
    "                 #  on this one\n",
    "               imputed_y_train_numbers_no_dates_times, y_train_bools, y_train_numbers_dates_times, y_train_lists = ums.drop_indexes_of_other_data_types(imputed_y_train_objects, \\\n",
    "                                                                                                                                                        imputed_y_train_numbers_no_dates_times, \\\n",
    "                                                                                                                                                        y_train_bools, \\\n",
    "                                                                                                                                                        y_train_numbers_dates_times, \\\n",
    "                                                                                                                                                        y_train_lists)                \n",
    "                \n",
    "           case 'A': #Mode imputation\n",
    "               imputed_y_train_objects = y_train_objects.fillna(y_train_objects.mode().iloc[0])\n",
    "\n",
    "    # Boolean missing data handling #\n",
    "    #\n",
    "    # Not required so far\n",
    "    # Also drop indexes of other types if the option is 'DEL'\n",
    "    imputed_y_train_bools = y_train_bools #\n",
    "\n",
    "    \n",
    "    # Datetime missing data handling (for interpolation) #\n",
    "    #\n",
    "    # Not required so far\n",
    "    # Also drop indexes of other types if the option is 'DEL'\n",
    "    imputed_y_train_numbers_dates_times = y_train_numbers_dates_times\n",
    "\n",
    "    \n",
    "    # List missing data handling\n",
    "    #\n",
    "    # Not required so far\n",
    "    # Each list-type feature handling might require a tailored approach\n",
    "    # Also drop indexes of other types if the option is 'DEL'\n",
    "    imputed_y_train_lists = y_train_lists \n",
    "    \n",
    "    #Rebuild the dataset\n",
    "    imputed_y_train = pd.concat([imputed_y_train_numbers_no_dates_times, \n",
    "                                 imputed_y_train_numbers_dates_times,\n",
    "                                 imputed_y_train_objects, imputed_y_train_bools,\n",
    "                                 imputed_y_train_lists], \n",
    "                                axis=1)\n",
    "    \n",
    "    if imputed_y_train.isna().sum().sum() == 0:\n",
    "        print(f\"No missing values left. Imputation successful! (y)\")\n",
    "\n",
    "    return imputed_y_train\n",
    "\n",
    "\n",
    "def detect_outliers(X_t, column_name, distribution_type, features_outlier_limits, features_in_wrong_distribution):\n",
    "    skewness = X_t[column_name].skew()\n",
    "    \n",
    "    # Based on a quick search, it seems the sknewness values mean:\n",
    "    #     Between [-0.5 , 0.5] -> fairly symmetrical, 'normal' distribution\n",
    "    #     Between [-1, -0.5] or [0.5, 1] -> fairly skewed, 'skewed' distrib.\n",
    "    #     <-1 or >1 -> very skewed -> 'skewed' (other?) distrib.\n",
    "        \n",
    "    match(distribution_type):\n",
    "        case 'normal': # Z-score\n",
    "            if not -0.5 < skewness < 0.5:\n",
    "                print(f\"\\nWarning: {column_name} is detecting outliers\\n\"\n",
    "                      f\"using a method more indicated for normal distributions.\\n\"\n",
    "                      f\"A skewness value of {skewness:.5f} indicates it is a skewed\\n\"\n",
    "                      f\"distribution, better suited for Interquantile Range.\\n\" \n",
    "                      f\"If you're sure you want it treated this way, comment\\n\" \n",
    "                      f\"the '.append()' and 'return' lines.\\n\" \n",
    "                      f\"It will be skipped for this method otherwise.\\n\")\n",
    "                #c#features_in_wrong_distribution.append(column_name)  \n",
    "                #c#return features_outlier_limits, features_in_wrong_distribution\n",
    "\n",
    "            # Calculate min/max values for acceptable Z-score #\n",
    "            #\n",
    "            #* 99.7% of the values either way. Edit at will the acceptable\n",
    "            #   thresholds.\n",
    "            zscores = [-3,3] #* \n",
    "            standard_deviation = X_t[column_name].std()\n",
    "            mean = X_t[column_name].mean()\n",
    "            min_nonoutlier_value = zscores[0]*standard_deviation + mean\n",
    "            max_nonoutlier_value = zscores[1]*standard_deviation + mean\n",
    "\n",
    "            features_outlier_limits[column_name] = [min_nonoutlier_value, max_nonoutlier_value]\n",
    "            \n",
    "            # Plot\n",
    "            #p#ums.plot_distribution_zscore(X_t, column_name, zscores, mean,\n",
    "            #                               standard_deviation, \n",
    "            #                               min_nonoutlier_value,\n",
    "            #                               max_nonoutlier_value)\n",
    "            \n",
    "    \n",
    "        case 'skewed': # IQR\n",
    "            if -0.5 < skewness < 0.5:\n",
    "                print(f\"\\nWarning: {column_name} is detecting outliers\\n\"\n",
    "                    f\"using a method more indicated for skewed distributions.\\n\"\n",
    "                    f\"A skewness value of {skewness:.5f} indicates it is a normal\\n\"\n",
    "                    f\"distribution, better suited for Z-score.\\n\" \n",
    "                    f\"If you're sure you want it treated this way, comment\\n\" \n",
    "                    f\"the '.append()' and 'return' lines.\\n\" \n",
    "                    f\"It will be skipped for this method otherwise.\\n\")\n",
    "                # Note: if the column is, in this moment, removed from \n",
    "                #       'features_to_remove_outliers_from', since that is the object\n",
    "                #       we iterating over, it will \"end\" the current loop and it \n",
    "                #       will also skip the next variable (why?), dangerous!\n",
    "                #\n",
    "                # Save the variable to safely remove it from the list later\n",
    "                features_in_wrong_distribution.append(column_name)  \n",
    "                return features_outlier_limits, features_in_wrong_distribution\n",
    "\n",
    "            #Calculate 25/75th percentiles, IQR, and limits\n",
    "            Q1 = X_t[column_name].quantile(0.25)\n",
    "            Q3 = X_t[column_name].quantile(0.75)\n",
    "            IQR = Q3-Q1\n",
    "            min_nonoutlier_value = Q1 - 1.5*IQR\n",
    "            max_nonoutlier_value = Q3 + 1.5*IQR\n",
    "\n",
    "            #p#print(f\"Feature: {column_name}\")\n",
    "            #p#print(f\"Q1: {Q1}\\nQ3: {Q3}\\nIQR: {IQR}\")\n",
    "            #p#print(f\"Min. non-outlier value: {min_nonoutlier_value}\")\n",
    "            #p#print(f\"Max. non-outlier value: {max_nonoutlier_value}\")\n",
    "\n",
    "            features_outlier_limits[column_name] = [min_nonoutlier_value, max_nonoutlier_value]\n",
    "            \n",
    "            # Plot\n",
    "            #p#ums.plot_distribution_IQR(X_t, column_name)\n",
    "\n",
    "\n",
    "        case 'other': #Percentile threshold\n",
    "            # Warning: IQR can go below the lowest number since it has a \"-1.5IQR\"\n",
    "            #         term but percentile cannot. Therefore if there are a lot of\n",
    "            #         minimum or maximum values (such as many 1's in \n",
    "            #         living_room/bathroom counts), the percentile's limit will\n",
    "            #         be at those values, deeming them outliers.\n",
    "            #         Use with care if maximum and minimum values in any feature\n",
    "            #         being analysed repeat themselves a lot.\n",
    "            #\n",
    "            # Will remove a big percentages of variables whose extremes are \n",
    "            #  very populated, such as \"1\" in the '__room_count' variables\n",
    "            percentile_threshold = 0.01  \n",
    "            min_nonoutlier_value = X_t[column_name].quantile(percentile_threshold)\n",
    "            max_nonoutlier_value = X_t[column_name].quantile(1-percentile_threshold)\n",
    "\n",
    "            #p#print(f\"Feature: {column_name}\")\n",
    "            #p#print(f\"Min. non-outlier value: {min_nonoutlier_value}\")\n",
    "            #p#print(f\"Max. non-outlier value: {max_nonoutlier_value}\")\n",
    "            \n",
    "            features_outlier_limits[column_name] = [min_nonoutlier_value, max_nonoutlier_value]\n",
    "\n",
    "            # Plot\n",
    "            #p#ums.plot_distribution_percentile(X_t, column_name, \n",
    "            #                                  percentile_threshold,\n",
    "            #                                  min_nonoutlier_value,\n",
    "            #                                  max_nonoutlier_value)\n",
    "            \n",
    "    return features_outlier_limits, features_in_wrong_distribution\n",
    "\n",
    "\n",
    "def treat_outliers(removed_outliers_X_t, outlier_treating_method, features_to_remove_outliers_from, features_outlier_limits):\n",
    "\n",
    "    match(outlier_treating_method):\n",
    "        case 0: # Custom method # Ignores distribution_type\n",
    "            #\n",
    "            # Note: these still represent \"natural\" possibilities within the home\n",
    "            #       population of the city in question. They will be removed for\n",
    "            #       testing purposes nontheless.\n",
    "            #\n",
    "            # Remove the largest price, which had a decent gap to the second largest\n",
    "            column_name = 'price'\n",
    "            if column_name in removed_outliers_X_t.columns:\n",
    "                removed_outliers_X_t = removed_outliers_X_t.drop(index=removed_outliers_X_t['price'].nlargest(n=1).index)\n",
    "            # Remove the northenmost latitude (coordinate_x) value which had a\n",
    "            #  decent gap to the second largest\n",
    "            column_name = 'coordinate_x'\n",
    "            if column_name in removed_outliers_X_t.columns:\n",
    "                removed_outliers_X_t = removed_outliers_X_t.drop(index=removed_outliers_X_t['coordinate_x'].nlargest(n=1).index)\n",
    "            # There's only a few id's below \"62000000\", lowest being in the\n",
    "            # 48000000's which will affect feature scaling, so they'll be dropped.\n",
    "            column_name = 'home_liverpool_id'\n",
    "            if column_name in removed_outliers_X_t.columns:\n",
    "                removed_outliers_X_t = removed_outliers_X_t.drop(index=removed_outliers_X_t[removed_outliers_X_t['home_liverpool_id']<62000000].index)\n",
    "\n",
    "            word = 'expert-elicited'\n",
    "            \n",
    "    \n",
    "        case 1: # Trimming\n",
    "            for column_name in features_to_remove_outliers_from:\n",
    "                removed_outliers_X_t = removed_outliers_X_t[ (removed_outliers_X_t[column_name]>features_outlier_limits[column_name][0]) \n",
    "                                                             &\n",
    "                                                             (removed_outliers_X_t[column_name]<features_outlier_limits[column_name][1]) ]\n",
    "                word = 'trimmed'\n",
    "                \n",
    "                \n",
    "        case 2: # Capping\n",
    "            for column_name in features_to_remove_outliers_from:\n",
    "                # Cap min\n",
    "                removed_outliers_X_t[column_name] = np.where(removed_outliers_X_t[column_name]<features_outlier_limits[column_name][0],\n",
    "                                                             features_outlier_limits[column_name][0],\n",
    "                                                             removed_outliers_X_t[column_name])    \n",
    "                # Cap max\n",
    "                removed_outliers_X_t[column_name] = np.where(removed_outliers_X_t[column_name]>features_outlier_limits[column_name][1],\n",
    "                                                             features_outlier_limits[column_name][1],\n",
    "                                                             removed_outliers_X_t[column_name]) \n",
    "\n",
    "                word = 'capped'\n",
    "                \n",
    "    '''\n",
    "    # Post outlier treatment plot of the suitable features\n",
    "    for column_name in features_to_remove_outliers_from:\n",
    "        plot1 = sns.histplot(removed_outliers_X_t[column_name], bins=30, edgecolor='black', kde=True)\n",
    "        plot1.set_xlabel(f\"{column_name}\")\n",
    "        plot1.set_ylabel(f\"Count\")\n",
    "        plot1.set_title(f\"{column_name} histogram, ({word} outliers)\", y=1.02)\n",
    "        plt.show()\n",
    "        plot1.figure.savefig(f\"{word}_outliers_{column_name}\")\n",
    "    '''\n",
    "    \n",
    "    return removed_outliers_X_t\n",
    "\n",
    "\n",
    "def remove_outliers(X_t, distribution_type, outlier_treating_method, features_to_remove_outliers_from='all'):\n",
    "    \n",
    "    if features_to_remove_outliers_from =='all':\n",
    "        features_to_remove_outliers_from = X_t.columns\n",
    "\n",
    "    \n",
    "    numeric_and_not_id_date_time_column = []\n",
    "    for column_name in features_to_remove_outliers_from:\n",
    "        # It usually doesnt make sense for date/time measures like months and days\n",
    "        #  to have outliers. Additionally, in the context of the collected listings\n",
    "        #  data, neither does it for 'year'\n",
    "        # Ignore non numeric features and dates/times\n",
    "        if np.issubdtype(X_t[column_name], np.number) and not (ums.is_date_or_time_column_name(column_name) or ums.is_year_column_name(column_name)):\n",
    "            numeric_and_not_id_date_time_column.append(column_name)\n",
    "    features_to_remove_outliers_from = numeric_and_not_id_date_time_column\n",
    "    \n",
    "    \n",
    "    # If we detect and remove outliers of a feature in the same iteration\n",
    "    #  the next iteration (feature) will have its detection influenced by the\n",
    "    #  removal of the values of the first. Therefore the limits will be simply\n",
    "    #  stored and the outliers removed alltogether at the end.\n",
    "    features_outlier_limits={}\n",
    "    features_in_wrong_distribution=[]\n",
    "    for column_name in features_to_remove_outliers_from:    \n",
    "        #------------------\n",
    "        # Detect outliers (max and min values) #\n",
    "        #------------------    \n",
    "        features_outlier_limits, features_in_wrong_distribution = detect_outliers(X_t,\n",
    "                                                                                  column_name,\n",
    "                                                                                  distribution_type, \n",
    "                                                                                  features_outlier_limits,\n",
    "                                                                                  features_in_wrong_distribution)\n",
    "        \n",
    "    #p#print(f\"features_outlier_limits:\\n{features_outlier_limits}\")\n",
    "    \n",
    "    # Ignore features used in the less appropriate distribution based on sknewness\n",
    "    for f in features_in_wrong_distribution: \n",
    "        features_to_remove_outliers_from.remove(f)\n",
    "\n",
    "    #------------------\n",
    "    # Remove outliers #\n",
    "    #------------------\n",
    "    removed_outliers_X_t = X_t.copy(deep=True)\n",
    "    removed_outliers_X_t = treat_outliers(removed_outliers_X_t, outlier_treating_method, \n",
    "                                          features_to_remove_outliers_from,\n",
    "                                          features_outlier_limits)\n",
    "    \n",
    "    \n",
    "    print(f\"Outliers successfully removed!\\nCurrent X_t dims: {removed_outliers_X_t.shape}\")\n",
    "    print(f\"NÂº of outliers removed: {X_t.shape[0] - removed_outliers_X_t.shape[0]}\"\n",
    "          f\" (0 is fine if the method was 'Capping'!)\")\n",
    "    return removed_outliers_X_t\n",
    "\n",
    "\n",
    "\n",
    "def encode_dataframe_data(X_t, X_train, y_train, encoding_method, drop_method=None, base_n=16, kfolds=5, smoothing='auto'):\n",
    "   \n",
    "    X_t_numbers_no_dates_times, X_t_numbers_dates_times, X_t_objects, X_t_bools, X_t_lists = ums.split_dataframe_data_by_type(X_t)\n",
    "    X_train_numbers_no_dates_times, X_train_numbers_dates_times, X_train_objects, X_train_bools, X_train_lists = ums.split_dataframe_data_by_type(X_train) \n",
    "    \n",
    "    # Encode non date/time numeric data (not applicable, only here for the sake \n",
    "    #  of clarity)\n",
    "    encoded_X_t_numbers_no_dates_times = X_t_numbers_no_dates_times\n",
    "\n",
    "    # Encode date/time numeric data\n",
    "    # Using the periodic functions of sin and cos:\n",
    "    for column_name in X_t_numbers_dates_times:\n",
    "        if ums.is_month_column_name(column_name):        \n",
    "            max_cardinality = ums.get_max_cardinality_of_months()\n",
    "            \n",
    "        elif ums.is_day_column_name(column_name):\n",
    "            max_cardinality = ums.get_max_cardinality_of_days()\n",
    "            \n",
    "        X_t_numbers_dates_times['encoded_'+column_name+'_sin'] = np.sin(2*np.pi * X_t_numbers_dates_times[column_name]/max_cardinality)\n",
    "        X_t_numbers_dates_times['encoded_'+column_name+'_cos'] = np.cos(2*np.pi * X_t_numbers_dates_times[column_name]/max_cardinality)\n",
    "        # Drop original (and non-cyclic representation of the) date/time column\n",
    "        X_t_numbers_dates_times.drop(columns=column_name, inplace=True)\n",
    "\n",
    "    encoded_X_t_numbers_dates_times = X_t_numbers_dates_times\n",
    "\n",
    "    # Encode strings (/objects)\n",
    "    match (encoding_method):\n",
    "        case 1: # OneHotEncoding\n",
    "            \n",
    "            # The \"unknown labels\" issue could be solved in 2 ways:\n",
    "            #  1 (and most correct?): handle_unknown='ignore' which will\n",
    "            #    encode the unknown categoricals (test set) as all 0's\n",
    "            #  2: Merge traint and test set to get all labels in each feature\n",
    "            #      as a list of lists and then pass that to the 'categories='\n",
    "            #      parameter.\n",
    "            #     However, what's the purpose of fitting with x_train if the\n",
    "            #      encoder will already know all the labels? It will then\n",
    "            #      encode the not seen ones of the test set with a '1' which\n",
    "            #      is erroneous and defeats the purpose of fitting with the\n",
    "            #      test set?\n",
    "            #      (This number 2 solution was seen at a towardsdatascience\n",
    "            #       'one-hot-encoding scikit vs pandas' post)\n",
    "            #\n",
    "            encoder = OneHotEncoder(drop=drop_method, handle_unknown='ignore', sparse_output=False)\n",
    "            encoder.fit(X_train_objects)\n",
    "            encoded_objects = encoder.transform(X_t_objects)\n",
    "            encoded_objects = pd.DataFrame(index=X_t_objects.index, \n",
    "                                           data=encoded_objects,\n",
    "                                           columns=encoder.get_feature_names_out())\n",
    "\n",
    "            \n",
    "        case 2: # Base N Encoding\n",
    "            encoder = ce.BaseNEncoder(base=base_n) \n",
    "            encoder.fit(X_train_objects)\n",
    "            encoded_objects = encoder.transform(X_t_objects)\n",
    "            \n",
    "\n",
    "        case 3: # Target Encoding\n",
    "            encoder = TargetEncoder(cv=kfolds, smooth=smoothing).set_output(transform=\"pandas\")\n",
    "            encoder.fit(X_train_objects, y_train)\n",
    "            encoded_objects = encoder.transform(X_t_objects)\n",
    "    \n",
    "    encoded_objects = encoded_objects.add_prefix('encoded_')\n",
    "    encoded_X_t_objects = encoded_objects\n",
    "    \n",
    "    \n",
    "    # Encode booleans\n",
    "    encoded_X_t_bools = X_t_bools.astype('int') #False-0, True-1\n",
    "    encoded_X_t_bools = encoded_X_t_bools.add_prefix('encoded_')\n",
    "    \n",
    "    # Encode lists  \n",
    "    # (In this case, they have already been dropped in\n",
    "    #  'handle_missing_dataframe_data_X(...)' by \"expert knowledge\". Reasons\n",
    "    #  explained there)\n",
    "    encoded_X_t_lists = X_t_lists\n",
    "   \n",
    "    encoded_X_t = pd.concat([encoded_X_t_numbers_no_dates_times, \n",
    "                             encoded_X_t_numbers_dates_times, encoded_X_t_objects,\n",
    "                             encoded_X_t_bools, encoded_X_t_lists],\n",
    "                            axis=1) \n",
    "    print(f\"Dims after encoding: {encoded_X_t.shape}\")\n",
    "    return encoded_X_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scale_dataframe_data(df_t, df_train, scaling_method):\n",
    "    # Avoid rescaling the following features:\n",
    "    #  - encoded categoricals\n",
    "    #  - dates/times, already encoded with periodic functions \n",
    "    \n",
    "    scaled_df_t = df_t.copy(deep=True)\n",
    "    scaler = None # In case no feature is numeric (only relevant for target var.) \n",
    "\n",
    "    match(scaling_method):\n",
    "        case 0:\n",
    "            scaler = 'No scaler'\n",
    "    \n",
    "        case 1: # Normalization (Min-max)\n",
    "            for column_name in scaled_df_t.columns:\n",
    "                #* Type check needed for when df_t = y_train \n",
    "                if not ums.is_encoded_column_name(column_name) and np.issubdtype(df_t[column_name].dtype, np.number): #*\n",
    "                    minmax_scaler = MinMaxScaler(feature_range=(0,1)) \n",
    "                    # Nans are treated as missing values, disregarded during fit\n",
    "                    #  and simply kept (ignored) during transform\n",
    "                    #* Pass as dataframe with '[[]]'\n",
    "                    minmax_scaler.fit(df_train[[column_name]]) #*\n",
    "                    scaled_df_t[column_name] = minmax_scaler.transform(scaled_df_t[[column_name]]) #*\n",
    "                    #p#print(f\"Min-max of {column_name}:\"\n",
    "                    #        f\" max:{scaled_X_t[column_name].nlargest(n=1).values},\"\n",
    "                    #        f\" min:{scaled_X_t[column_name].nsmallest(n=1).values}\")\n",
    "            scaler = minmax_scaler                     \n",
    "    \n",
    "        case 2: # Mean Normalisation\n",
    "            for column_name in scaled_df_t.columns:\n",
    "                if not ums.is_encoded_column_name(column_name) and np.issubdtype(df_t[column_name].dtype, np.number):\n",
    "                    # Avoid NaN's if min and max are the same.\n",
    "                    if scaled_df_t[column_name].max() - scaled_df_t[column_name].min() == 0:\n",
    "                        scaled_df_t[column_name] = 0\n",
    "                    else:\n",
    "                        scaled_df_t[column_name] = (scaled_df_t[column_name] - df_train[column_name].mean() ) / \\\n",
    "                                                    ( scaled_df_t[column_name].max() - scaled_df_t[column_name].min() )\n",
    "                    #p#print(f\"Min-max of {column_name}:\"\n",
    "                    #        f\" max:{encoded_X_t[column_name].nlargest(n=1).values},\"\n",
    "                    #        f\" min:{encoded_X_t[column_name].nsmallest(n=1).values}\")\n",
    "                    #p#print(f\"And mean: {encoded_X_train[column_name].mean()}\")\n",
    "            scaler = 'MeanNormalisation'\n",
    "            \n",
    "    \n",
    "        case 3: # Standardization\n",
    "            for column_name in scaled_df_t.columns:\n",
    "                if not ums.is_encoded_column_name(column_name) and np.issubdtype(df_t[column_name].dtype, np.number): \n",
    "                    standard_scaler = StandardScaler() \n",
    "                    standard_scaler.fit(df_train[[column_name]])\n",
    "                    scaled_df_t[column_name] = standard_scaler.transform(scaled_df_t[[column_name]])\n",
    "                    #p#print(f\"Min-max of {column_name}:\"\n",
    "                    #        f\" max:{scaled_X_t[column_name].nlargest(n=1).values},\"\n",
    "                    #        f\" min:{scaled_X_t[column_name].nsmallest(n=1).values}\")\n",
    "            scaler = standard_scaler\n",
    "\n",
    "\n",
    "        case 4: # Robust Scaler\n",
    "            for column_name in scaled_df_t.columns:\n",
    "                if not ums.is_encoded_column_name(column_name) and np.issubdtype(df_t[column_name].dtype, np.number): \n",
    "                    robust_scaler = RobustScaler() \n",
    "                    robust_scaler.fit(df_train[[column_name]])\n",
    "                    scaled_df_t[column_name] = robust_scaler.transform(scaled_df_t[[column_name]])\n",
    "                    #p#print(f\"Min-max of {column_name}:\" \n",
    "                    #        f\" max:{scaled_X_t[column_name].nlargest(n=1).values},\"\n",
    "                    #        f\" min:{scaled_X_t[column_name].nsmallest(n=1).values}\")\n",
    "            scaler = robust_scaler\n",
    "\n",
    "\n",
    "        case 5: # Power Transformer\n",
    "            for column_name in scaled_df_t.columns:\n",
    "                if not ums.is_encoded_column_name(column_name) and np.issubdtype(df_t[column_name].dtype, np.number): \n",
    "                    power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "                    power_transformer.fit(df_train[[column_name]])\n",
    "                    scaled_df_t[column_name] = power_transformer.transform(scaled_df_t[[column_name]])\n",
    "                    #print(f\"Min-max of {column_name}:\"\n",
    "                    #      f\" max:{scaled_X_t[column_name].nlargest(n=1).values},\"\n",
    "                    #      f\" min:{scaled_X_t[column_name].nsmallest(n=1).values}\")\n",
    "            scaler = power_transformer\n",
    "\n",
    "    # Return the scaler instance only if scaling (single target) y\n",
    "    #  It will be used to revert the scaling\n",
    "    if df_t.shape[1] == 1:\n",
    "        return scaled_df_t, scaler\n",
    "    else:\n",
    "        return scaled_df_t\n",
    "\n",
    "\n",
    "\n",
    "def dimensionally_reduce(X_t, X_train, y_train, dimensionality_reduction_method):\n",
    "    \n",
    "    match(dimensionality_reduction_method):\n",
    "        case 1: # Random Forests    \n",
    "            # Not ready for multi-target\n",
    "            if y_train.shape[1] > 1:\n",
    "                print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\"\n",
    "                      f\"(Feature selection via) Random Forests Classification/Regression\"\n",
    "                      f\" is not ready for multi target, please select another\"\n",
    "                      f\" dim.reduction method\\n\"\n",
    "                      f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                return\n",
    " \n",
    "            if np.issubdtype(y_train[y_train.columns[0]].dtype , np.number):\n",
    "                rf_model = RandomForestRegressor(n_jobs=-1, random_state=42)        \n",
    "            elif np.issubdtype(y_train[y_train.columns[0]].dtype , object):\n",
    "                rf_model = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "            rf_model.fit(X_train, y_train.values.ravel())\n",
    "            \n",
    "            feature_importances = pd.Series(rf_model.feature_importances_, index=X_t.columns).sort_values(ascending=False)\n",
    "            \n",
    "            # Visualize feature importances \n",
    "            # Plot#\n",
    "            '''\n",
    "            print(f\"\\nfeat_importances:\\n{feature_importances}\")\n",
    "            plot1 = sns.barplot(x=feature_importances[:15], y=feature_importances[:15].index)\n",
    "            plot1.set_title(\"Barchart on the top 15 feature importances for a prediction on...\")\n",
    "            plot1.set_xlabel('Feature importance score')\n",
    "            plot1.set_ylabel('Feature name')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plot1.figure.savefig(f\"rf_feature_importances\") \n",
    "            '''\n",
    "            \n",
    "            # Select features \n",
    "            #\n",
    "            #* Edit theshold at will. Default is 'mean'.\n",
    "            feature_importance_threshold = 'mean' #*\n",
    "            rf_model_selector = SelectFromModel(rf_model, threshold=feature_importance_threshold) \n",
    "            rf_model_selector.fit(X_train, y_train.values.ravel())\n",
    "            dimensionally_reduced_X_t = rf_model_selector.transform(X_t)\n",
    "              \n",
    "                \n",
    "        case 2: # PCA\n",
    "            # Get an initial PCA to find a \"good\" number of components\n",
    "            n_pca_components = 'default' \n",
    "            if n_pca_components == 'default':\n",
    "                # Reduce by 1 from min(rows/columns)\n",
    "                pca = PCA(n_components=min(X_train.shape[0], X_train.shape[1]) - 1) \n",
    "            else:\n",
    "                pca = PCA(n_components=n_pca_components) \n",
    "            try:\n",
    "                pca.fit(X_train)\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(f\"!!!!!!!!!!!!!!!ERROR!!!!!!!!!!!!!!!!!!!\\n\"\n",
    "                      f\"SVD may have failed to converge\"\n",
    "                      f\"It is unlikely that the cause is linked to NaN's\"\n",
    "                      f\" or 'inf.' values...\"\n",
    "                      f\"\\nReturning and exiting PCA feature selection\"\n",
    "                      f\"\\n!!!!!!!!!!!!!!!ERROR!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "                return'N/A'\n",
    "           \n",
    "            # Find that number of components by checking when enough variance is\n",
    "            #  explained\n",
    "            n_pca_components = None\n",
    "            explained_variances_ratios = pca.explained_variance_ratio_\n",
    "            explained_variances_ratios_cumsum = np.cumsum(explained_variances_ratios)\n",
    "\n",
    "            for i, cumsum in enumerate(explained_variances_ratios_cumsum):\n",
    "                #If an explained variance ratio of 0.9 is met, leave it at that\n",
    "                if cumsum >= 0.9 and i<99:\n",
    "                    n_pca_components = i+1\n",
    "                    break\n",
    "                #Otherwise get as many as necessary to \"explain\" 80% of the data\n",
    "                elif cumsum >=0.8 and i>=99: #while n_pca_components<100 try to reach 0.9\n",
    "                    n_pca_components = i+1\n",
    "                    break\n",
    "                    \n",
    "            # Plot for visualization\n",
    "            '''\n",
    "            print(f\"componen: {n_pca_components}\")\n",
    "            x_limit =100\n",
    "            plt.title(\"PCA's explained variance ratios barchart\")\n",
    "            plot1 = sns.barplot(x=range(0, len(explained_variances_ratios[:x_limit])), y=explained_variances_ratios[:x_limit], label='Explained variances ratios', align='center')\n",
    "            plot1.step(x=range(0, len(explained_variances_ratios_cumsum[:x_limit])), y=explained_variances_ratios_cumsum[:x_limit], label='Explained variances ratios cumul.sum.', where='mid')\n",
    "            plot1.set_xticks(np.arange(0, x_limit, step=5))\n",
    "            plot1.set_xlabel(\"Explained variance ratio index\")\n",
    "            plot1.set_ylabel(\"Explained variance ratio\")\n",
    "            plot1.legend(loc='best')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plot1.figure.savefig(f\"pca_explained_variance_ratios\")\n",
    "            '''\n",
    "            print(f\"Number of components and corresponding explained variance\"\n",
    "                  f\" ratio:\\n{n_pca_components} -\" \n",
    "                  f\" {explained_variances_ratios_cumsum[n_pca_components-1]}\")\n",
    "            \n",
    "            # Build a PCA model with the obtained \"recommended\" number\n",
    "            #  of components\n",
    "            pca = PCA(n_components = n_pca_components)\n",
    "            pca.fit(X_train)\n",
    "            \n",
    "            dimensionally_reduced_X_t = pca.transform(X_t)\n",
    "\n",
    "    \n",
    "        case 3: #LDA\n",
    "            # Not ready for multi-target\n",
    "            if y_train.shape[1] > 1:\n",
    "                print(f\"!!!!!!!!!!!!!!!ERROR!!!!!!!!!!!!!!!!!!!!\\n\"\n",
    "                      f\"LDA is not ready for multi target, please select\" \n",
    "                      f\" another dim.reduction method\\n\"\n",
    "                      f\"Returning and exiting...\\n\"\n",
    "                      f\"!!!!!!!!!!!!!!!ERROR!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "                return \n",
    "                \n",
    "            # Not meant for numeric \"labels\"\n",
    "            if not np.issubdtype(y_train[y_train.columns[0]].dtype, object):\n",
    "                print(f\"!!!!!!!!!!!!!!!ERROR!!!!!!!!!!!!!!!!!!!\\n\"\n",
    "                      f\"The target variable is of type:\"\n",
    "                      f\" {y_train[y_train.columns[0]].dtype}\\n\"\n",
    "                      f\"LDA accepts only the feature/column type: \\\"object\\\".\\n\" \n",
    "                      f\"Returning and exiting...\\n\"\n",
    "                      f\"!!!!!!!!!!!!!!!ERROR!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "                return 'N/A'\n",
    "                \n",
    "            num_predictors = X_train.shape[1] # Number features            \n",
    "            #* Number of labels of the target\n",
    "            num_target_classes = y_train[y_train.columns[0]].nunique() #*\n",
    "            lda = LinearDiscriminantAnalysis(n_components=min(num_predictors, num_target_classes-1))\n",
    "\n",
    "            try:\n",
    "                lda.fit(X_train, y_train.values.ravel())\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(f\"!!!!!!!!!!!!!!!ERROR!!!!!!!!!!!!!!!!!!!\\n\"\n",
    "                      f\"SVD may have failed to converge\"\n",
    "                      f\"It is unlikely that the cause is linked to NaN's or\"\n",
    "                      f\" 'inf.' values...\"\n",
    "                      f\"\\nReturning and exiting LDA feature selection\"\n",
    "                      f\"!!!!!!!!!!!!!!!ERROR!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "                return'N/A'\n",
    "            \n",
    "            dimensionally_reduced_X_t = lda.transform(X_t)\n",
    "    \n",
    "            # Plot #\n",
    "            '''\n",
    "            explained_variances_ratios = lda.explained_variance_ratio_\n",
    "            print(f\"a\\n {explained_variances_ratios}\")\n",
    "            explained_variances_ratios_cumsum = np.cumsum(explained_variances_ratios)\n",
    "            plt.title(\"LDA's explained variance ratios barchart\")\n",
    "            plot1 = sns.barplot(x=range(0, len(explained_variances_ratios)), y=explained_variances_ratios, label='Explained variances ratios', align='center')\n",
    "            plot1.step(x=range(0, len(explained_variances_ratios_cumsum)), y=explained_variances_ratios_cumsum, label='Explained variances ratios cumul.sum.', where='mid')\n",
    "            plot1.set_xticks(np.arange(0, len(explained_variances_ratios), step=5))\n",
    "            plot1.set_xlabel(\"Explained variance ratio index\")\n",
    "            plot1.set_ylabel(\"Explained variance ratio\")\n",
    "            plot1.legend(loc='best')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            #plot1.figure.savefig(f\"lda_explained_variance_ratios\")\n",
    "            '''\n",
    "\n",
    "    \n",
    "        case 4: #Truncated SVD\n",
    "            # Choose the number of components based on plotted explained variance:\n",
    "            #  - Build a T_SVD with all the features to see the number of them\n",
    "            #    we need to get the desired explained variance\n",
    "            # Note: gives at most min(rows, columns) components (so no point\n",
    "            #       putting anything above that in the hyerparameter)\n",
    "            analysis_truncated_svd = TruncatedSVD(n_components=min(X_train.shape[0], X_train.shape[1]), algorithm='randomized', random_state=42)\n",
    "            analysis_truncated_svd.fit(X_train)\n",
    "        \n",
    "            explained_variance_percent = analysis_truncated_svd.explained_variance_ratio_ * 100 \n",
    "            explained_variance_percent_cumsum = np.cumsum(explained_variance_percent)\n",
    "            \n",
    "            # Plot 1 #\n",
    "            # Explained variance and comulative sum WITH intermidiate numbers\n",
    "            #  being displayed\n",
    "            '''\n",
    "            plt.bar(range(1, min(X_train.shape[0], X_train.shape[1])+1), explained_variance_percent, align='center', label='Explained variance by component (%)', color='lightblue')\n",
    "            # Add numbers in the graph at each (x,y) for clarity \n",
    "            for i in range(1, min(X_train.shape[0], X_train.shape[1])+1, 2):\n",
    "                plt.text(i, explained_variance_percent[i-1]*1.01, round(explained_variance_percent[i-1], 2), ha='center', \n",
    "                            bbox=dict(facecolor='yellow', boxstyle=BoxStyle('square', pad=0.2), alpha=0.8))  \n",
    "            \n",
    "            plt.step(range(1, min(X_train.shape[0], X_train.shape[1])+1), explained_variance_percent_cumsum, label='Cumulative explained variance', color='darkblue')\n",
    "            # Add numbers in the graph at each (x,y) for clarity\n",
    "            for i in range(1, min(X_train.shape[0], X_train.shape[1])+1, 2):\n",
    "                plt.text(i, explained_variance_percent_cumsum[i-1]*1.01, round(explained_variance_percent_cumsum[i-1], 2), ha='center',\n",
    "                        bbox=dict(facecolor='orange', boxstyle=BoxStyle('square', pad=0.2), alpha=0.8)) \n",
    "            \n",
    "            plt.xlabel('Component index')\n",
    "            plt.ylabel('Explained variance percentage')\n",
    "            plt.xticks(ticks=list(range(1, X_train.shape[1]+1)))\n",
    "            plt.legend(loc='best') \n",
    "            plt.tight_layout() \n",
    "            plt.show()\n",
    "             \n",
    "            '''\n",
    "           \n",
    "            # Plot 2 #\n",
    "            # Simplified explained variance and comulative sum WITHOUT \n",
    "            #  intermidiate numbers being displayed\n",
    "            '''\n",
    "            explained_variances_ratios = analysis_truncated_svd.explained_variance_ratio_\n",
    "            explained_variances_ratios_cumsum = np.cumsum(explained_variances_ratios)\n",
    "\n",
    "            plt.title(\"Truncated SVD's explained variance ratios barchart\")\n",
    "            plot2 = sns.barplot(x=range(0, len(explained_variances_ratios)), y=explained_variances_ratios, label='Explained variances ratios', align='center')\n",
    "            plot2.step(x=range(0, len(explained_variances_ratios_cumsum)), y=explained_variances_ratios_cumsum, label='Explained variances ratios cumul.sum.', where='mid')\n",
    "            plot2.set_xticks(np.arange(0, len(explained_variances_ratios), step=5))\n",
    "            plot2.set_xlabel(\"Explained variance ratio index\")\n",
    "            plot2.set_ylabel(\"Explained variance ratio\")\n",
    "            plot2.legend(loc='best')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            #plot2.figure.savefig(f\"truncated_SVD_explained_variance_ratios\")\n",
    "            '''\n",
    "\n",
    "            # Select number of components based on threshold of explained \n",
    "            #  variance (ratio (*100)) \n",
    "            explained_variance_percent_threshold = 99\n",
    "            n_necessary_components = 0 \n",
    "            for i in range(len(explained_variance_percent_cumsum)):\n",
    "                if explained_variance_percent_cumsum[i] >= explained_variance_percent_threshold:\n",
    "                    n_necessary_components = i + 1\n",
    "                    break\n",
    "\n",
    "            # Now reduce the dimensionality based on the selected\n",
    "            #  features/components\n",
    "            truncated_svd = TruncatedSVD(n_components=n_necessary_components, \n",
    "                                         algorithm='randomized', random_state=42)\n",
    "            truncated_svd.fit(X_train)\n",
    "            dimensionally_reduced_X_t = truncated_svd.transform(X_t)\n",
    "\n",
    "    \n",
    "        case 5: #Kernel PCA\n",
    "            kernel_pca = KernelPCA(n_components=100, kernel='poly', random_state=42, n_jobs=-1)\n",
    "            kernel_pca.fit(X_train)\n",
    "            dimensionally_reduced_X_t = kernel_pca.transform(X_t)\n",
    "            \n",
    "            # Plot #\n",
    "            '''\n",
    "            plt.title(\"Explained variance ratios barchart\")\n",
    "            plot1 = sns.barplot(x=range(0, len(kernel_pca.eigenvalues_)), y=kernel_pca.eigenvalues_, align='center')\n",
    "            #plot1.step(x=range(0, len(kernel_pca.eigenvalues_)), y=explained_variances_ratios_cumsum[:x_limit], label='Explained variances ratios cumul.sum.', where='mid')\n",
    "            plot1.set_xticks(np.arange(0, len(kernel_pca.eigenvalues_), step=5))\n",
    "            plot1.set_xlabel(\"Eigenvalue index\")\n",
    "            plot1.set_ylabel(\"Eigenvalues\")\n",
    "            #plot1.legend(loc='best')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plot1.figure.savefig(f\"kernel_pca_top100_eigenvalues\")\n",
    "            '''\n",
    "           \n",
    "            \n",
    "    return dimensionally_reduced_X_t\n",
    "\n",
    "\n",
    "##########################\n",
    "### Data preprocessing ###\n",
    "##########################\n",
    "def preprocess_dataframes_data(dfs_to_merge, preprocessing_settings):\n",
    "    \n",
    "    #############\n",
    "    # Settings: #\n",
    "    #############\n",
    "    #\n",
    "    # Default target for one-off predictions\n",
    "    target_variable_name = preprocessing_settings['target_variable_name'] \n",
    "    #* For selecting which variables will be filled\n",
    "    missing_data_threshold = preprocessing_settings['missing_data_threshold'] #*\n",
    "    features_to_drop_expert_knowledge = preprocessing_settings['features_to_drop_expert_knowledge']\n",
    "    #* For splitting train/test data based on the Nan's of the target \n",
    "    #  (fill missing data with predictions)\n",
    "    is_nan_splitting = preprocessing_settings['is_nan_splitting']#*\n",
    "    numeric_imputation_method_X = preprocessing_settings['numeric_imputation_method_X'] \n",
    "    categorical_imputation_method_X = preprocessing_settings['categorical_imputation_method_X']\n",
    "    numeric_imputation_method_y = preprocessing_settings['numeric_imputation_method_y']\n",
    "    categorical_imputation_method_y = preprocessing_settings['categorical_imputation_method_y']\n",
    "    distribution_type = preprocessing_settings['distribution_type']\n",
    "    outlier_treating_method = preprocessing_settings['outlier_treating_method']\n",
    "    encoding_method = preprocessing_settings['encoding_method']\n",
    "    scaling_method = preprocessing_settings['scaling_method']\n",
    "    dimensionality_reduction_method = preprocessing_settings['dimensionality_reduction_method'] \n",
    "\n",
    "    \n",
    "    ###############\n",
    "    # Data Cleaning \n",
    "    ###############\n",
    "    #\n",
    "    # There are no equivalent tables in the scrapped datasets to \"group\" or\n",
    "    #  remove typos, different spelling, value-formatting, etc... \n",
    "    #\n",
    "    # The non-existence of duplicates was ensured by SQL 'ON UPDATE' clauses\n",
    "    #  during the insertion\n",
    "    #\n",
    "    # No erroneous data was originated from the scrapping\n",
    "    #\n",
    "    # Let's ensure the variables are in the right types\n",
    "    #\n",
    "    cleaned_dfs_to_merge = clean_dataframes_data(dfs_to_merge)\n",
    "\n",
    "    ##################\n",
    "    # Data Integration\n",
    "    ##################\n",
    "    #\n",
    "    # Merge the data from the web scrapped tables to create the \"full\" dataset\n",
    "    #\n",
    "    integrated_df = integrate_dataframes_data(cleaned_dfs_to_merge) \n",
    "        \n",
    "    ############################################\n",
    "    #Split the data into training and test sets:\n",
    "    ############################################\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_dataframe_data_train_test(integrated_df,\n",
    "                                                                       target_variable_name,\n",
    "                                                                       is_nan_splitting)\n",
    "    print(f\"Initial dims: X_train: {X_train.shape}, X_test: {X_test.shape},\"\n",
    "          f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
    "    \n",
    "    ##############################################################\n",
    "    # Handle (impute/remove) missing data (before training models)\n",
    "    ##############################################################\n",
    "    #\n",
    "    # Any feature columns that are not deemed useful based on expert knowledge\n",
    "    #  can be instantly removed\n",
    "    # For now, those will consist of:\n",
    "    #  - 'real_estate_agent_id': Home_liverpool_id is for the most part\n",
    "    #                            connected with the listing date, so it\n",
    "    #                            might be a useful source of information\n",
    "    #                            for the algorithm for predictions, being\n",
    "    #                            added as an exception to the \n",
    "    #                            is_id_column_name() method). The estate \n",
    "    #                            agent id, on the other hand, isn't, so it\n",
    "    #                            can be discarded.\n",
    "    #\n",
    "    #  - 'photos': list of urls which would have to be encoded and most likely \n",
    "    #              with no value\n",
    "    #\n",
    "    #  - 'feature_set': given the nature of the listings, this list can contain\n",
    "    #                   features written in a completely arbitrary way, leading\n",
    "    #                   to even similar features having different encodings later\n",
    "    #                   on in the preprocessing steps. Adding to the apparent\n",
    "    #                   impracticality of encoding a list, the result could also\n",
    "    #                   be misleading for the model.\n",
    "    #\n",
    "    #  - 'description': (similar reasoning to 'feature_set')\n",
    "    #\n",
    "    #\n",
    "    # Features with missing data ratio over a certain threshold will be removed\n",
    "    #\n",
    "    # Datetime and bools had a NOT NULL clause in the databse, hence not needed\n",
    "    # This leaves Numerical and Categorical (numpy object) data\n",
    "    \n",
    "    for column_name in X_train.columns:\n",
    "        if ums.is_id_column_name(column_name):\n",
    "            features_to_drop_expert_knowledge.add(column_name)\n",
    "\n",
    "\n",
    "    if is_nan_splitting == True:\n",
    "        if numeric_imputation_method_X == 'DEL':\n",
    "            print(f\"WARNING: numeric_imputation_method_X will be changed from\"\n",
    "                  f\" 'DEL' to 1 to avoid y_test row losses.\")\n",
    "            numeric_imputation_method_X = 1\n",
    "        if categorical_imputation_method_X == 'DEL':\n",
    "            print(f\"WARNING: categorical_imputation_method_X will be changed from\"\n",
    "                  f\" 'DEL' to 'A' to avoid y_test row losses.\")\n",
    "            categorical_imputation_method_X = 'A'  \n",
    "\n",
    "    # X imputation/deletion\n",
    "    imputed_X_train = handle_missing_dataframe_data_X(\n",
    "        X_train, X_train, missing_data_threshold, features_to_drop_expert_knowledge,\n",
    "        numeric_imputation_method_X, categorical_imputation_method_X\n",
    "    )\n",
    "    imputed_X_test = handle_missing_dataframe_data_X(\n",
    "        X_train, X_test, missing_data_threshold, features_to_drop_expert_knowledge,\n",
    "        numeric_imputation_method_X, categorical_imputation_method_X\n",
    "    )   \n",
    "    # If deletion method, update target (y) to match X!\n",
    "    if numeric_imputation_method_X == 'DEL' or categorical_imputation_method_X == 'DEL':\n",
    "        y_train = y_train.loc[imputed_X_train.index.to_list()]\n",
    "        y_test = y_test.loc[imputed_X_test.index.to_list()] \n",
    "\n",
    "    # y imputation/deletion #\n",
    "    y_train = handle_missing_dataframe_data_y(y_train, numeric_imputation_method_y,\n",
    "                                              categorical_imputation_method_y)\n",
    "    # No imputation method should be used on the testing target, simply drop Nans.\n",
    "    if is_nan_splitting == False:\n",
    "        y_test.dropna(inplace=True) \n",
    "    \n",
    "    # If deletion method, update predictor (X) to match y! #\n",
    "    if numeric_imputation_method_y == 'DEL' or categorical_imputation_method_y == 'DEL':\n",
    "        imputed_X_train = imputed_X_train.loc[y_train.index.to_list()]\n",
    "    # Outside of the 'if' since y_test drops its \"NaN\"s rehardless\n",
    "    imputed_X_test = imputed_X_test.loc[y_test.index.to_list()]  \n",
    "\n",
    "\n",
    "    ########################\n",
    "    # Detect/Remove Outliers\n",
    "    ########################    \n",
    "\n",
    "    # Edit and add as the 4th parameter of `remove_outliers()` if only a\n",
    "    #  specific set of features is to have outliers treated.\n",
    "    #  By default, all numeric ones will be adressed \n",
    "    features_to_remove_outliers_from = [] \n",
    "    removed_outliers_X_train = remove_outliers(imputed_X_train, distribution_type,\n",
    "                                               outlier_treating_method) \n",
    "    \n",
    "    removed_outliers_X_test = imputed_X_test\n",
    "    \n",
    "   \n",
    "    # Plotting distributions before/after outlier removal\n",
    "    '''\n",
    "    for column_name in removed_outliers_X_train.columns:\n",
    "        if np.issubdtype(removed_outliers_X_train[column_name].dtype, np.number):\n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.subplot(2,2,1)\n",
    "            plt.title(f\"{column_name} value distribution\")\n",
    "            sns.histplot(imputed_X_train[column_name])\n",
    "            plt.subplot(2,2,2)\n",
    "            sns.boxplot(data=imputed_X_train[column_name])\n",
    "            #---------------------------------------------------\n",
    "            plt.subplot(2,2,3)\n",
    "            plt.title(f\"{column_name} value distribution (w/o outliers, method: {outlier_treating_method}\")\n",
    "            sns.histplot(removed_outliers_X_train[column_name])\n",
    "            plt.subplot(2,2,4)\n",
    "            sns.boxplot(data=removed_outliers_X_train[column_name])\n",
    "\n",
    "            plt.subplots_adjust(hspace=0.4)\n",
    "            plt.show() \n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Update target (y)\n",
    "    # Remove any deleted rows from the target variable as well \n",
    "    # Note: Indexes MUST be PRESERVED so they can MATCH \n",
    "    y_train = y_train.loc[removed_outliers_X_train.index.to_list()]\n",
    "    y_test = y_test.loc[removed_outliers_X_test.index.to_list()]\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    # Encode the (categorical, boolean and datetime) data\n",
    "    #####################################################\n",
    "    # Cyclic Feature Encoding will be used on cyclic features extracted from\n",
    "    #  the original datetime feature. In this case, month and day will be encoded\n",
    "    #  to preserve the relationship between the \"extreme\" values (month 1<->12, \n",
    "    #  day 2nd<->28th etc \n",
    "    #\n",
    "    # Booleans will simply be converted to type 'int'\n",
    "    \n",
    "\n",
    "\n",
    "    encoded_X_train = encode_dataframe_data(removed_outliers_X_train,\n",
    "                                            removed_outliers_X_train,\n",
    "                                            y_train, encoding_method)\n",
    "    encoded_X_test = encode_dataframe_data(removed_outliers_X_test,\n",
    "                                           removed_outliers_X_train,\n",
    "                                           y_train, encoding_method)\n",
    "   \n",
    "    \n",
    "    #################\n",
    "    #Feature Scaling\n",
    "    #################\n",
    "\n",
    "    #Values to retain to revert the scaling of numeric predictions:\n",
    "    y_test_mins, y_test_maxs, y_train_means = ums.get_values_for_scaling_reversion(y_test,\n",
    "                                                                                   y_train)\n",
    "    \n",
    "\n",
    "    # Scale target variable(s) (if applicable) and get scaler to revert it on the\n",
    "    #  prediction results\n",
    "    if ums.has_numeric_feature(y_train): \n",
    "        # Scale test first else (scaled)y_train will be used in it fitting due\n",
    "        #  to same var name ':)\n",
    "        #\n",
    "        # 'scaler' gets assigned on both these 'y_' since the scaling function\n",
    "        #  is set to always return it. It's not an issue in any case, since \n",
    "        #  it's the same. \n",
    "        y_test, scaler = scale_dataframe_data(y_test, y_train, scaling_method) \n",
    "        y_train, scaler = scale_dataframe_data(y_train, y_train, scaling_method) \n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    # Scale predictors (if applicable)\n",
    "    if ums.has_numeric_feature(encoded_X_train): \n",
    "        scaled_X_train = scale_dataframe_data(encoded_X_train, encoded_X_train,\n",
    "                                              scaling_method)\n",
    "        scaled_X_test = scale_dataframe_data(encoded_X_test, encoded_X_train,\n",
    "                                             scaling_method)\n",
    "    else:\n",
    "        scaled_X_train = encoded_X_train\n",
    "        scaled_X_test = encoded_X_test\n",
    "        \n",
    "    scaler_info = [scaler, y_test_mins, y_test_maxs, y_train_means]\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    # Dimensionality Reduction\n",
    "    #\n",
    "    # (Feature Selection and Feature Projection)\n",
    "    ############################################\n",
    "\n",
    "    dimensionally_reduced_X_train = dimensionally_reduce(scaled_X_train,\n",
    "                                                         scaled_X_train,\n",
    "                                                         y_train,\n",
    "                                                         dimensionality_reduction_method)\n",
    "    dimensionally_reduced_X_test = dimensionally_reduce(scaled_X_test,\n",
    "                                                        scaled_X_train,\n",
    "                                                        y_train,\n",
    "                                                        dimensionality_reduction_method)\n",
    "    # Variable could be of type 'str' if there was an 'N/A' return\n",
    "    #  due to, for instance, a failure of the model to converge\n",
    "    #  with the current parameters\n",
    "    if type(dimensionally_reduced_X_train) is not str: \n",
    "        print(f\"dimensionally_reduced_X_train shape:\\n{dimensionally_reduced_X_train.shape}\")\n",
    "    \n",
    "    return (dimensionally_reduced_X_train, dimensionally_reduced_X_test,\n",
    "            y_train, y_test, scaler_info)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebScrapProj_RealEstate",
   "language": "python",
   "name": "webscrapproj_realestate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
